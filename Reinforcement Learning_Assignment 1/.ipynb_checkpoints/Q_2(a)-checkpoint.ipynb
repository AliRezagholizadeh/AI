{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Creating_Dynamics(n):\n",
    "    \n",
    "    return\n",
    "\n",
    "def Setting_Rewards():\n",
    "    \n",
    "    Rewards = [0, 1, 10]\n",
    "    \n",
    "    return Rewards\n",
    "\n",
    "def Setting_Actions():\n",
    "    Actions = [\"U\", \"D\", \"L\", \"R\"]\n",
    "    \n",
    "    return Actions\n",
    "\n",
    "def Creating_Stochastic_Policy(n):\n",
    "    \n",
    "    return\n",
    "\n",
    "def Convert_position_list_to_str(s):\n",
    "    # s is [,] position.\n",
    "    s_str = '['\n",
    "    s_str += ''.join(str(x) for x in s)\n",
    "    s_str += ']'\n",
    "    \n",
    "    return s_str\n",
    "def Convert_position_str_to_list(s_str):\n",
    "    \n",
    "    r = int(s_str[1])\n",
    "    c = int(s_str[2])\n",
    "    \n",
    "    s= [r,c]\n",
    "    \n",
    "    return s\n",
    "\n",
    "def greedy_policy_of_current_policy(S_b, S_t, V):\n",
    "    import random\n",
    "    Greedy_Policy = {}\n",
    "    for s in S_b:\n",
    "        Action_set = Deterministic_Actions(s,V)\n",
    "        if(Action_set == \"Error\"):\n",
    "            print(\"Error occurred in obtaining greedy policy in state: \", s)\n",
    "        else:\n",
    "            s_str = Convert_position_list_to_str(s)\n",
    "            if(len(Action_set) > 0):\n",
    "                n_a = len(Action_set)\n",
    "                direction_n = int(random.uniform(0,n_a))\n",
    "                direction = Action_set[direction_n]\n",
    "                d = {s_str: direction}\n",
    "                Greedy_Policy.update(d)\n",
    "                \n",
    "    for s in S_t:\n",
    "        s_str = Convert_position_list_to_str(s)\n",
    "        d = {s_str: \"\"}\n",
    "        Greedy_Policy.update(d)\n",
    "                \n",
    "        \n",
    "    \n",
    "    # Greedy_Policy  is a dict  like: {\"[00]\":'U'}\n",
    "    return Greedy_Policy\n",
    "    \n",
    "\n",
    "def Determine_States(n):\n",
    "    # This is determined according the requirements of the ass1 _ q2(a) in Comp 767.\n",
    "    # the states are position like [[,][,]...]\n",
    "    S_t = [[0,0] , [0,n-1]]\n",
    "    # S_t is terminal state.\n",
    "    S_b = [[i,j] for i in range(n) for j in range(n)]\n",
    "    S_b.pop(n-1)\n",
    "    S_b.pop(0)\n",
    "    # S_b is in between _ not terminal state.\n",
    "    \n",
    "    return S_t, S_b\n",
    "    \n",
    "\n",
    "def Inital_value_function(S_b,S_t):\n",
    "    # S_t  is terminal states,  and 'S_b'  is between sates. Each is list of positions. [[,] [,] ..]\n",
    "    import math\n",
    "    n1 = len(S_b)\n",
    "    n2 = len(S_t)\n",
    "    \n",
    "    n = n1+n2\n",
    "    width = int(math.sqrt(n))\n",
    "    Value_Function = []\n",
    "    for i in range(width):\n",
    "        r = [0 for j in range(width)]\n",
    "        Value_Function.append(r)\n",
    "    \n",
    "    return Value_Function\n",
    "    \n",
    "\n",
    "def Deterministic_Actions(s,V):\n",
    "    # this is deterministic action selection according to policy Pai.\n",
    "    # So, it select an action according to value function of that policy.\n",
    "    # input: s is a position of one point, list of two numbers (position in gride- which is starting from 0)    -  \n",
    "    #        ... V is Value function of a policy (is matrix of gride size).\n",
    "    import numpy as np\n",
    "    r = s[0]\n",
    "    c = s[1]\n",
    "    \n",
    "    All_logical_neighbors = [[r-1, c], [r+1 , c], [r, c-1] , [r, c+1]]\n",
    "    end_of_grid = len(V)\n",
    "    Possible_neighbors = [z for z in All_logical_neighbors if(z[0]>-1 and z[0]< end_of_grid and z[1]>-1 and z[1]< end_of_grid)]\n",
    "    max_neighbor_value = -10\n",
    "    \n",
    "    for position in Possible_neighbors:\n",
    "        value = V[position[0]][position[1]]\n",
    "        if(max_neighbor_value < value):\n",
    "            max_neighbor_value = value\n",
    "    \n",
    "    max_neighbor_pos = []\n",
    "    for position in Possible_neighbors:\n",
    "        value = V[position[0]][position[1]]\n",
    "        if(max_neighbor_value == value):\n",
    "            max_neighbor_pos.append(position)\n",
    "    # all neighbors who have the max value would enter to 'max_neighbor_pos'.\n",
    "    \n",
    "    action_set = \"\"\n",
    "    s_array = np.array(s)\n",
    "    for max_Neigh in max_neighbor_pos:\n",
    "        max_neighbor_arr = np.array(max_Neigh)\n",
    "        move = max_neighbor_arr - s_array\n",
    "    \n",
    "        if(move[0] == -1):\n",
    "            action_set += \"U\"\n",
    "        elif(move[0] == 1):\n",
    "            action_set += \"D\"\n",
    "        else:\n",
    "            if(move[0] != 0):\n",
    "                print(\"Error in Choosing_deterministic_Action, 1.\", move[0])\n",
    "                action_set = \"Error\"\n",
    "\n",
    "\n",
    "        if(move[1] == -1):\n",
    "            action_set += \"L\"\n",
    "        elif(move[1] == 1):\n",
    "            action_set += \"R\"\n",
    "        else:\n",
    "            if(move[1] != 0):\n",
    "                print(\"Error in Choosing_deterministic_Action, 2.\" , move[1])\n",
    "                action_set = \"Error\"\n",
    "\n",
    "\n",
    "    \n",
    "    return action_set\n",
    "    \n",
    "\n",
    "def Printing_ValueFunction(matrix_list):\n",
    "    # 'matrix_list' is [[,,..][,,..]...]\n",
    "    r_len = len(matrix_list)\n",
    "    c_len = len(matrix_list[0])\n",
    "    print(\"Value Function:\", end ='\\n')\n",
    "    for i in range(r_len):\n",
    "        for j in range(c_len):\n",
    "            if(j==0):\n",
    "                print(end=' | ')\n",
    "                print(matrix_list[i][j], end=' | ')\n",
    "            else:\n",
    "                print(matrix_list[i][j], end=' | ')\n",
    "        print('')\n",
    "    return\n",
    "\n",
    "def Printing_Deterministic_Policy(g_p,S_t, S_b):\n",
    "    # g_p is dict of like {'[11]': \"UDLR\"}\n",
    "    import math\n",
    "    n1 = len(S_b)\n",
    "    n2 = len(S_t)\n",
    "    \n",
    "    n = n1+n2\n",
    "    width = int(math.sqrt(n))\n",
    "\n",
    "    Greedy_Policy_mat = []\n",
    "    for i in range(width):\n",
    "        r = ['' for j in range(width)]\n",
    "        Greedy_Policy_mat.append(r)  \n",
    "    \n",
    "    for s in S_b:\n",
    "        s_str = Convert_position_list_to_str(s)\n",
    "        Greedy_Policy_mat[s[0]][s[1]] = g_p[s_str]\n",
    "        \n",
    "    \n",
    "    for s in S_t:\n",
    "        #s_str = Convert_position_list_to_str(s)\n",
    "        Greedy_Policy_mat[s[0]][s[1]] = \"term\"\n",
    "     \n",
    "\n",
    "    # printing \n",
    "    matrix_list = Greedy_Policy_mat\n",
    "    r_len = len(matrix_list)\n",
    "    c_len = len(matrix_list[0])\n",
    "\n",
    "    #'value %3s - num of occurances = %d' % item\n",
    "    #'{0:10} ==> {1:10d}'.format(name, phone)\n",
    "    \n",
    "    \n",
    "    for i in range(r_len):\n",
    "        for j in range(c_len):\n",
    "            if(j==0):\n",
    "                print(end=' | ')\n",
    "                print(\"%5s |\"% matrix_list[i][j], end='')\n",
    "            else:\n",
    "                print(\"%5s |\"% matrix_list[i][j], end='')\n",
    "        print('')\n",
    "    \n",
    "    \n",
    "    return\n",
    "\n",
    "def Printing_Stochastic_Policy():\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "#==============================================================================================\n",
    "# DP part:\n",
    "# \n",
    "\n",
    "\n",
    "def Policy_Iteration(S_b, S_t, Rewards, Actions, Policy, V, Discount, P_Dynamic, e):\n",
    "    import math\n",
    "    Greedy_Policy = Policy\n",
    "    # Greedy_Policy  is dict of like {'[12]':'U'}\n",
    "    Theta = 0.001\n",
    "    \n",
    "    S_p = S_b + S_t\n",
    "    n = len(S_p)\n",
    "    n = int(math.sqrt(n))\n",
    "    policy_stable = False\n",
    "    # S_p is s+\n",
    "    Number_of_update_policy = 1\n",
    "    V_S_Left = []\n",
    "    V_S_Right = []\n",
    "    while(policy_stable == False):\n",
    "        \n",
    "        print(\"===================================== update: \",Number_of_update_policy,\" ==================================\")\n",
    "        print(\"Greedy policy of current policy: \\n\")\n",
    "        Printing_Deterministic_Policy(Greedy_Policy,S_t, S_b)\n",
    "        \n",
    "        print(\"Computing V_i through the value function of aforementioned greedy policy ... ... .\")\n",
    "        Delta = 2\n",
    "        while(Theta < Delta):\n",
    "            Delta = 0\n",
    "            for s in S_b:\n",
    "                # s is [r,c] as the position of current state.\n",
    "                r_s = s[0]\n",
    "                c_s = s[1]\n",
    "                current_v = V[r_s][c_s]\n",
    "                s_str = Convert_position_list_to_str(s)\n",
    "                greedy_action = Greedy_Policy[s_str]\n",
    "                \n",
    "                Greedy_Policy_ = E_greedy_policy(Greedy_Policy, S_b, Actions, e)\n",
    "                actions, p_ = Greedy_Policy_[s_str]\n",
    "                if(greedy_action != \"\"):\n",
    "                    # when s is not terminal\n",
    "                    sum__ = 0\n",
    "                    for a_i in range(len(Actions)):\n",
    "                        pi = p_[a_i]\n",
    "                        ac = Actions[a_i]\n",
    "                        sum_ = 0\n",
    "                        for s_ in S_p:\n",
    "                            for r in Rewards:\n",
    "                                p_dy = Dynamic_p(s_,r,s,ac, P_Dynamic)\n",
    "                                if(p_dy != 0):\n",
    "                                    #print(\"s, greedy_action, S_, r, p\",s, greedy_action, s_, r , p_dy)\n",
    "                                    sum_ += p_dy*(r + Discount* V[s_[0]][s_[1]])\n",
    "                        sum__ += pi * sum_\n",
    "\n",
    "                    new_v = sum__\n",
    "                    V[r_s][c_s] = new_v\n",
    "                    diff = abs(current_v - new_v)\n",
    "                    #print(\"diff: \", diff, end=' - ')\n",
    "                    Delta = max(Delta , diff)\n",
    "            #print()\n",
    "            #print(\"Theta and Delta: \",Theta , Delta, end=' - ')\n",
    "        print(\"Value_function obtained from 'Iterative policy evaluation' is: \")\n",
    "        Printing_ValueFunction(V)\n",
    "        V_S_Left.append(V[n-1][0])\n",
    "        V_S_Right.append(V[n-1][n-1])\n",
    "        print(\"---------------------\")\n",
    "        print(\"Now updating greedy policy using this Value_function achieved ... ..\")\n",
    "        policy_stable = True\n",
    "        for s in S_b:\n",
    "            s_str = Convert_position_list_to_str(s)\n",
    "            current_action = Greedy_Policy[s_str]\n",
    "            \n",
    "            max_action_value = 0\n",
    "            max_q_action = \"\"\n",
    "            for a in Actions:\n",
    "                sum_ = 0\n",
    "                for s_ in S_p:\n",
    "                    for r in Rewards:\n",
    "                        p_dy = Dynamic_p(s_,r,s,a, P_Dynamic)\n",
    "                        if(p_dy != 0):\n",
    "                            sum_ += p_dy*(r + Discount* V[s_[0]][s_[1]])\n",
    "                \n",
    "                if(max_action_value <= sum_):\n",
    "                    max_action_value = sum_\n",
    "                    max_q_action = a\n",
    "                    \n",
    "            \n",
    "            Greedy_Policy[s_str] = max_q_action\n",
    "            \n",
    "            if(current_action != max_q_action):\n",
    "                policy_stable = False\n",
    "            \n",
    "        print(\"Greedy policy of updated policy: \\n\")\n",
    "        Printing_Deterministic_Policy(Greedy_Policy,S_t, S_b)\n",
    "        if(policy_stable == False):\n",
    "            Number_of_update_policy += 1\n",
    "            print(\"Upadted policy is not stable so we go through obtaining its Value_fuction, then updating g_policy again. \")\n",
    "            print(\"=========================================================================================\")\n",
    "            print(\"|||||||||||||||||||||||||||||||||||||||\")\n",
    "        else:\n",
    "            print(\"No update was needed. The last one is the optimal policy.\")\n",
    "\n",
    "\n",
    "    \n",
    "        \n",
    "    return V_S_Left, V_S_Right, Number_of_update_policy\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------\n",
    "def E_greedy_policy(Greedy_Policy, S_b, Actions, e):\n",
    "    Greedy_Policy_ = {}\n",
    "    for s in S_b:\n",
    "        s_str = Convert_position_list_to_str(s)\n",
    "        greedy_action = Greedy_Policy[s_str]\n",
    "        indx = 0\n",
    "        index = -1\n",
    "        for ac in Actions:\n",
    "            if(ac == greedy_action):\n",
    "                index = indx\n",
    "            indx += 1\n",
    "        p = [0 for i in range(len(Actions))]\n",
    "        p[index] = e\n",
    "        e_ = (1-e)/4\n",
    "        p_ = [i + e_ for i in p]\n",
    "        r = {s_str: [Actions, p_]}\n",
    "        Greedy_Policy_.update(r)\n",
    "    \n",
    "    return Greedy_Policy_\n",
    "def E_greedy(Greedy_Policy, S_b, Actions, e):\n",
    "    import random\n",
    "    \n",
    "    for s in S_b:\n",
    "        s_str = Convert_position_list_to_str(s)\n",
    "        greedy_action = Greedy_Policy[s_str]\n",
    "        \n",
    "        t = random.uniform(0,1)\n",
    "        if(t<e):\n",
    "            selection = greedy_action\n",
    "        else:\n",
    "            t_2 = random.uniform(0,1)\n",
    "            if(t_2 < 0.25):\n",
    "                selection = Actions[0]\n",
    "            elif(t_2 < 0.5):\n",
    "                selection = Actions[1]\n",
    "            elif(t_2 < 0.75):\n",
    "                selection = Actions[2]\n",
    "            else:\n",
    "                selection = Actions[3]\n",
    "        \n",
    "    return selection\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "# Regarding Dynamic:\n",
    "def Possible_Action_for_states(S_b,S_t):\n",
    "    import numpy as np\n",
    "    Possible_Actions_for_states = {}\n",
    "    terminal_1_str = Convert_position_list_to_str(S_t[0])\n",
    "    terminal_2_str = Convert_position_list_to_str(S_t[1])\n",
    "    for s in S_b:\n",
    "        r = s[0]\n",
    "        c = s[1]\n",
    "\n",
    "        All_logical_neighbors = [[r-1, c], [r+1 , c], [r, c-1] , [r, c+1]]\n",
    "        end_of_grid = len(V)\n",
    "        Possible_neighbors = [z for z in All_logical_neighbors if(z[0]>-1 and z[0]< end_of_grid and z[1]>-1 and z[1]< end_of_grid)]\n",
    "        max_neighbor_value = -10\n",
    "\n",
    "        s_array = np.array(s)\n",
    "        action_set = \"\"\n",
    "        Next_state_set =[]\n",
    "        Rewards = []\n",
    "        s_array = np.array(s)\n",
    "        for pos_N in Possible_neighbors:\n",
    "            pos_neighbor_arr = np.array(pos_N)\n",
    "            move = pos_neighbor_arr - s_array\n",
    "\n",
    "            if(move[0] == -1):\n",
    "                action_set += \"U\"\n",
    "                N_state_str = Convert_position_list_to_str(pos_N)\n",
    "                Next_state_set.append(N_state_str)\n",
    "                if(N_state_str == terminal_1_str):\n",
    "                    Rewards.append(1)\n",
    "                elif(N_state_str == terminal_2_str):\n",
    "                    Rewards.append(10)\n",
    "                else:\n",
    "                    Rewards.append(0)\n",
    "            elif(move[0] == 1):\n",
    "                action_set += \"D\"\n",
    "                N_state_str = Convert_position_list_to_str(pos_N)\n",
    "                Next_state_set.append(N_state_str)  \n",
    "                if(N_state_str == terminal_1_str):\n",
    "                    Rewards.append(1)\n",
    "                elif(N_state_str == terminal_2_str):\n",
    "                    Rewards.append(10)\n",
    "                else:\n",
    "                    Rewards.append(0)\n",
    "            else:\n",
    "                if(move[0] != 0):\n",
    "                    print(\"Error in Choosing_deterministic_Action, 1.\", move[0])\n",
    "                    action_set = \"Error\"\n",
    "\n",
    "\n",
    "            if(move[1] == -1):\n",
    "                action_set += \"L\"\n",
    "                N_state_str = Convert_position_list_to_str(pos_N)\n",
    "                Next_state_set.append(N_state_str) \n",
    "                if(N_state_str == terminal_1_str):\n",
    "                    Rewards.append(1)\n",
    "                elif(N_state_str == terminal_2_str):\n",
    "                    Rewards.append(10)\n",
    "                else:\n",
    "                    Rewards.append(0)                \n",
    "            elif(move[1] == 1):\n",
    "                action_set += \"R\"                \n",
    "                N_state_str = Convert_position_list_to_str(pos_N)\n",
    "                Next_state_set.append(N_state_str)     \n",
    "                if(N_state_str == terminal_1_str):\n",
    "                    Rewards.append(1)\n",
    "                elif(N_state_str == terminal_2_str):\n",
    "                    Rewards.append(10)\n",
    "                else:\n",
    "                    Rewards.append(0)\n",
    "            else:\n",
    "                if(move[1] != 0):\n",
    "                    print(\"Error in Choosing_deterministic_Action, 2.\" , move[1])\n",
    "                    action_set = \"Error\"\n",
    "        \n",
    "        s_str = Convert_position_list_to_str(s)\n",
    "        state_action = {s_str:[action_set, Next_state_set, Rewards]}\n",
    "        Possible_Actions_for_states.update(state_action)\n",
    "    for s in S_t: \n",
    "        action_set = \"\"\n",
    "        Next_state_set = []\n",
    "        Rewards = []\n",
    "        s_str = Convert_position_list_to_str(s)\n",
    "        state_action = {s_str:[action_set, Next_state_set, Rewards]}\n",
    "        Possible_Actions_for_states.update(state_action)\n",
    "    return Possible_Actions_for_states\n",
    "\n",
    "def Dynamic_p(s_,r,s,a , P):\n",
    "    # s is like [1,1] ; a is like \"U\" ; r is like 0 ; s_ is like [3,3]\n",
    "    # P is like: {'[01]':[Actions, Next_state, Rewards]}  \n",
    "    # S_b\n",
    "    s_str = Convert_position_list_to_str(s)\n",
    "    Actions, Next_state, Rewards = P[s_str]\n",
    "    #print(Actions)\n",
    "    indx = 0\n",
    "    index = -1\n",
    "    for ac in Actions:\n",
    "        if(ac == a):\n",
    "            index = indx\n",
    "        indx += 1\n",
    "    \n",
    "    # now index is corresponding action to take, next state, next reward considering this action.\n",
    "    if(index == -1):\n",
    "        return 0\n",
    "    s__str = Convert_position_list_to_str(s_)\n",
    "    if(Next_state[index] != s__str):\n",
    "        return 0\n",
    "    if(Rewards[index] != r):\n",
    "        return 0\n",
    "    \n",
    "    return 1\n",
    "    \n",
    "    # for each 'current state', 'action' and 'reward' , 'new state'  it should return the dynamic  of \n",
    "# -------------------------------------------------------------------------------------------------\n",
    "\n",
    "def Value_Iteration():\n",
    "    \n",
    "    return\n",
    "\n",
    "def Modified_Policy_Iteration():\n",
    "    \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " |  term |    R |    L |    D | term |\n",
      " |     D |    U |    D |    U |    U |\n",
      " |     U |    U |    U |    L |    U |\n",
      " |     R |    L |    R |    R |    D |\n",
      " |     R |    U |    L |    U |    L |\n"
     ]
    }
   ],
   "source": [
    "S_t, S_b = Determine_States(5)\n",
    "V = Inital_value_function(S_b,S_t)\n",
    "g_Policy = greedy_policy_of_current_policy(S_b, S_t, V)\n",
    "Printing_Deterministic_Policy(g_Policy,S_t, S_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_for_run(g_Policy):\n",
    "    S_t, S_b = Determine_States(5)\n",
    "    Rewards = Setting_Rewards()\n",
    "    Actions = Setting_Actions()\n",
    "    V = Inital_value_function(S_b,S_t)\n",
    "    e = 0.7\n",
    "    Discount = 0.9\n",
    "    P = Possible_Action_for_states(S_b,S_t)  # this is created for specific question\n",
    "    V_S_Left, V_S_Right, Number_of_update_policy = Policy_Iteration(S_b, S_t, Rewards, Actions, g_Policy, V, Discount, P, e)\n",
    "    \n",
    "    import matplotlib.pyplot as plt \n",
    "  \n",
    "    # x axis values \n",
    "    x = [i-1 for i in range(Number_of_update_policy)] \n",
    "    # corresponding y axis values \n",
    "    y_1 = V_S_Left \n",
    "    # plotting the points  \n",
    "    plt.plot(x, y_1, label = \"Botton left\")  \n",
    "\n",
    "    y_2 = V_S_Right\n",
    "    plt.plot(x, y_2, label = \"Botton Right\")\n",
    "    # naming the x axis \n",
    "    plt.xlabel('number of updates') \n",
    "    # naming the y axis \n",
    "    plt.ylabel('value function') \n",
    "\n",
    "    # giving a title to my graph \n",
    "    plt.title('Value function of two bottom states.') \n",
    "\n",
    "    # function to show the plot \n",
    "    plt.show() \n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================== update:  1  ==================================\n",
      "Greedy policy of current policy: \n",
      "\n",
      " |  term |    R |    L |    D | term |\n",
      " |     D |    U |    D |    U |    U |\n",
      " |     U |    U |    U |    L |    U |\n",
      " |     R |    L |    R |    R |    D |\n",
      " |     R |    U |    L |    U |    L |\n",
      "Computing V_i through the value function of aforementioned greedy policy ... ... .\n",
      "Value_function obtained from 'Iterative policy evaluation' is: \n",
      "Value Function:\n",
      " | 0 | 0.4781989468308125 | 0.5386079706950442 | 2.4374238299087083 | 0 | \n",
      " | 0.24325074511965142 | 0.41407240286400837 | 0.6019042754612866 | 2.3675881891041337 | 8.308638814214943 | \n",
      " | 0.201677452896884 | 0.34926829065730647 | 0.5423917608945362 | 0.9787345641941076 | 5.908629701827957 | \n",
      " | 0.12611954346648255 | 0.15278099691207592 | 0.48874784840210833 | 0.6192350699041428 | 0.700805581600465 | \n",
      " | 0.09402332127755918 | 0.12303420697857699 | 0.1503282059459617 | 0.46724181285386757 | 0.373205541223604 | \n",
      "---------------------\n",
      "Now updating greedy policy using this Value_function achieved ... ..\n",
      "Greedy policy of updated policy: \n",
      "\n",
      " |  term |    L |    R |    R | term |\n",
      " |     U |    R |    R |    R |    U |\n",
      " |     R |    R |    R |    R |    U |\n",
      " |     U |    R |    R |    U |    U |\n",
      " |     U |    U |    U |    U |    U |\n",
      "Upadted policy is not stable so we go through obtaining its Value_fuction, then updating g_policy again. \n",
      "=========================================================================================\n",
      "|||||||||||||||||||||||||||||||||||||||\n",
      "===================================== update:  2  ==================================\n",
      "Greedy policy of current policy: \n",
      "\n",
      " |  term |    L |    R |    R | term |\n",
      " |     U |    R |    R |    R |    U |\n",
      " |     R |    R |    R |    R |    U |\n",
      " |     U |    R |    R |    U |    U |\n",
      " |     U |    U |    U |    U |    U |\n",
      "Computing V_i through the value function of aforementioned greedy policy ... ... .\n",
      "Value_function obtained from 'Iterative policy evaluation' is: \n",
      "Value Function:\n",
      " | 0 | 1.5536706395666453 | 6.604033142221356 | 8.702134684533698 | 0 | \n",
      " | 1.3354438405977387 | 4.932016458700775 | 6.361878589451791 | 7.501675117808501 | 8.71866898408692 | \n",
      " | 3.3717510845850382 | 4.439150325238656 | 5.1968641489313665 | 5.979030313583749 | 6.848979351291084 | \n",
      " | 2.7487587198667596 | 3.7615751360569685 | 4.406498462990058 | 5.106953598050385 | 5.394422112584759 | \n",
      " | 2.120181878321018 | 3.00653075507954 | 3.5514916232537708 | 4.0743654664006 | 4.03762909250991 | \n",
      "---------------------\n",
      "Now updating greedy policy using this Value_function achieved ... ..\n",
      "Greedy policy of updated policy: \n",
      "\n",
      " |  term |    R |    R |    R | term |\n",
      " |     R |    R |    R |    R |    U |\n",
      " |     R |    R |    U |    U |    U |\n",
      " |     R |    U |    U |    U |    U |\n",
      " |     R |    U |    U |    U |    U |\n",
      "Upadted policy is not stable so we go through obtaining its Value_fuction, then updating g_policy again. \n",
      "=========================================================================================\n",
      "|||||||||||||||||||||||||||||||||||||||\n",
      "===================================== update:  3  ==================================\n",
      "Greedy policy of current policy: \n",
      "\n",
      " |  term |    R |    R |    R | term |\n",
      " |     R |    R |    R |    R |    U |\n",
      " |     R |    R |    U |    U |    U |\n",
      " |     R |    U |    U |    U |    U |\n",
      " |     R |    U |    U |    U |    U |\n",
      "Computing V_i through the value function of aforementioned greedy policy ... ... .\n",
      "Value_function obtained from 'Iterative policy evaluation' is: \n",
      "Value Function:\n",
      " | 0 | 5.240563464507472 | 6.8757449842960865 | 8.72367334706943 | 0 | \n",
      " | 4.156656364838824 | 5.477900398942854 | 6.477778919658295 | 7.549054089499167 | 8.724716574518922 | \n",
      " | 3.8665541400723504 | 4.820533157708164 | 5.6067168120011095 | 6.481008790275556 | 6.89119414491285 | \n",
      " | 3.3167862807948607 | 4.135781396111602 | 4.823984297669992 | 5.510166494761648 | 5.455355951036767 | \n",
      " | 2.53817211395434 | 3.3182334639793614 | 3.8845174890528487 | 4.382357857170958 | 4.1009199312071845 | \n",
      "---------------------\n",
      "Now updating greedy policy using this Value_function achieved ... ..\n",
      "Greedy policy of updated policy: \n",
      "\n",
      " |  term |    R |    R |    R | term |\n",
      " |     R |    R |    R |    R |    U |\n",
      " |     R |    R |    R |    U |    U |\n",
      " |     R |    R |    U |    U |    U |\n",
      " |     R |    U |    U |    U |    U |\n",
      "Upadted policy is not stable so we go through obtaining its Value_fuction, then updating g_policy again. \n",
      "=========================================================================================\n",
      "|||||||||||||||||||||||||||||||||||||||\n",
      "===================================== update:  4  ==================================\n",
      "Greedy policy of current policy: \n",
      "\n",
      " |  term |    R |    R |    R | term |\n",
      " |     R |    R |    R |    R |    U |\n",
      " |     R |    R |    R |    U |    U |\n",
      " |     R |    R |    U |    U |    U |\n",
      " |     R |    U |    U |    U |    U |\n",
      "Computing V_i through the value function of aforementioned greedy policy ... ... .\n",
      "Value_function obtained from 'Iterative policy evaluation' is: \n",
      "Value Function:\n",
      " | 0 | 5.240623692019464 | 6.875770725797501 | 8.723677747369322 | 0 | \n",
      " | 4.15713443181664 | 5.4782712952759205 | 6.478004330140658 | 7.549087966242451 | 8.724720328172683 | \n",
      " | 3.868471262745751 | 4.822753674319615 | 5.60921422092457 | 6.481225239340325 | 6.891214214569798 | \n",
      " | 3.320184286311268 | 4.140057756732122 | 4.826159522265379 | 5.510491529734551 | 5.455395517722329 | \n",
      " | 2.5407716913806406 | 3.3215135465574344 | 3.886280506700611 | 4.382707200994954 | 4.1009711096784836 | \n",
      "---------------------\n",
      "Now updating greedy policy using this Value_function achieved ... ..\n",
      "Greedy policy of updated policy: \n",
      "\n",
      " |  term |    R |    R |    R | term |\n",
      " |     R |    R |    R |    R |    U |\n",
      " |     R |    R |    R |    U |    U |\n",
      " |     R |    R |    U |    U |    U |\n",
      " |     R |    U |    U |    U |    U |\n",
      "No update was needed. The last one is the optimal policy.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "g_Policy_ = g_Policy\n",
    "main_for_run(g_Policy_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================== update:  1  ==================================\n",
      "Greedy policy of current policy: \n",
      "\n",
      " |  term |    R |    R |    D |    L |    D |    D |    L |    L |    D |    R |    L |    D |    R |    D |    D |    L |    R |    D |    R |    D |    L |    L |    D |    R |    L |    L |    R |    L |    L |    R |    D |    D |    L |    L |    R |    D |    R |    R |    R |    L |    R |    R |    L |    R |    L |    L |    D |    D | term |\n",
      " |     R |    U |    L |    D |    L |    R |    D |    R |    D |    U |    U |    R |    D |    R |    L |    L |    R |    R |    L |    D |    D |    L |    D |    R |    R |    R |    R |    L |    L |    L |    U |    D |    U |    D |    L |    R |    L |    R |    L |    L |    R |    R |    U |    R |    R |    L |    L |    L |    L |    U |\n",
      " |     U |    U |    L |    U |    L |    L |    R |    D |    U |    U |    R |    R |    L |    U |    R |    U |    L |    U |    R |    R |    R |    U |    U |    D |    D |    L |    D |    L |    R |    L |    D |    U |    R |    L |    U |    R |    R |    R |    U |    R |    R |    D |    D |    D |    R |    U |    U |    U |    L |    U |\n",
      " |     D |    D |    U |    D |    L |    D |    L |    U |    D |    U |    R |    D |    U |    L |    U |    D |    U |    R |    L |    R |    D |    U |    R |    U |    L |    U |    L |    D |    D |    U |    D |    R |    U |    D |    L |    L |    L |    D |    U |    D |    D |    U |    L |    L |    L |    L |    R |    D |    U |    D |\n",
      " |     R |    D |    R |    U |    U |    R |    R |    L |    R |    L |    U |    U |    R |    D |    U |    R |    D |    U |    R |    U |    U |    D |    U |    D |    U |    D |    D |    R |    D |    D |    U |    R |    R |    U |    U |    U |    R |    U |    L |    U |    D |    R |    L |    U |    D |    U |    L |    R |    L |    L |\n",
      " |     R |    R |    R |    L |    U |    R |    R |    L |    D |    R |    U |    L |    U |    U |    U |    D |    L |    L |    R |    D |    L |    R |    U |    L |    R |    L |    D |    R |    L |    D |    D |    R |    L |    L |    U |    U |    D |    D |    L |    L |    D |    D |    U |    L |    L |    R |    R |    L |    D |    U |\n",
      " |     R |    R |    D |    U |    L |    U |    D |    D |    D |    U |    R |    D |    R |    D |    L |    R |    U |    D |    R |    D |    R |    L |    L |    R |    R |    R |    U |    D |    L |    L |    D |    D |    L |    U |    R |    D |    R |    R |    R |    R |    L |    R |    D |    U |    R |    L |    R |    R |    D |    D |\n",
      " |     U |    D |    L |    L |    L |    R |    R |    D |    R |    U |    U |    U |    D |    U |    D |    R |    U |    D |    U |    U |    U |    L |    R |    L |    R |    L |    L |    D |    U |    D |    U |    L |    R |    U |    R |    L |    U |    U |    R |    R |    D |    D |    L |    L |    R |    D |    R |    R |    R |    U |\n",
      " |     D |    R |    R |    L |    D |    D |    D |    R |    U |    U |    U |    U |    R |    D |    L |    R |    L |    L |    U |    R |    L |    U |    U |    R |    U |    U |    U |    D |    L |    U |    U |    L |    U |    U |    U |    R |    R |    D |    R |    R |    D |    D |    R |    D |    U |    D |    R |    U |    L |    L |\n",
      " |     R |    L |    L |    U |    U |    D |    D |    U |    D |    R |    D |    L |    D |    R |    U |    R |    R |    D |    R |    D |    L |    L |    U |    R |    U |    D |    L |    U |    R |    R |    R |    L |    R |    L |    U |    R |    L |    D |    L |    D |    L |    D |    R |    R |    R |    U |    D |    R |    D |    L |\n",
      " |     R |    D |    D |    R |    R |    L |    R |    D |    L |    D |    L |    D |    D |    L |    R |    D |    U |    L |    L |    D |    L |    L |    R |    R |    D |    L |    R |    L |    D |    U |    R |    R |    L |    L |    U |    D |    L |    R |    R |    D |    D |    D |    D |    L |    D |    R |    D |    U |    L |    L |\n",
      " |     U |    R |    D |    R |    L |    L |    R |    R |    L |    D |    R |    R |    U |    U |    D |    D |    U |    U |    R |    R |    U |    D |    D |    R |    D |    L |    D |    L |    D |    R |    L |    R |    U |    U |    L |    L |    L |    R |    D |    L |    D |    D |    R |    R |    R |    R |    L |    D |    R |    L |\n",
      " |     D |    L |    D |    R |    R |    R |    R |    L |    L |    L |    L |    D |    D |    R |    D |    L |    L |    U |    L |    D |    U |    R |    L |    R |    R |    U |    D |    D |    R |    D |    D |    D |    U |    R |    D |    U |    D |    U |    L |    D |    R |    U |    U |    D |    U |    U |    D |    L |    L |    L |\n",
      " |     U |    D |    U |    D |    L |    R |    L |    R |    L |    L |    L |    D |    U |    D |    R |    D |    U |    U |    U |    U |    R |    U |    L |    D |    D |    L |    D |    L |    D |    R |    U |    R |    R |    L |    U |    L |    R |    D |    D |    D |    R |    D |    L |    R |    L |    R |    U |    R |    D |    U |\n",
      " |     R |    R |    U |    R |    R |    L |    L |    L |    L |    U |    L |    R |    L |    U |    D |    L |    L |    U |    L |    L |    U |    U |    L |    D |    D |    D |    D |    L |    L |    R |    U |    R |    U |    D |    U |    D |    L |    R |    R |    D |    U |    D |    L |    D |    R |    L |    D |    L |    U |    D |\n",
      " |     R |    R |    L |    U |    L |    U |    L |    R |    D |    R |    U |    D |    R |    L |    L |    D |    R |    D |    R |    U |    D |    U |    U |    L |    D |    D |    R |    L |    D |    R |    U |    D |    U |    L |    U |    U |    R |    D |    R |    L |    L |    D |    L |    U |    R |    L |    U |    D |    L |    D |\n",
      " |     R |    D |    D |    L |    D |    D |    U |    L |    L |    L |    D |    D |    R |    L |    L |    R |    L |    U |    U |    L |    L |    L |    U |    D |    L |    U |    L |    U |    D |    U |    U |    L |    U |    U |    L |    R |    U |    D |    R |    U |    L |    L |    R |    D |    U |    D |    L |    D |    L |    U |\n",
      " |     D |    R |    U |    R |    L |    U |    U |    R |    U |    R |    D |    U |    U |    U |    D |    R |    R |    R |    U |    D |    L |    D |    D |    D |    U |    R |    L |    L |    L |    U |    U |    L |    U |    D |    L |    D |    L |    R |    U |    R |    L |    U |    L |    R |    D |    D |    L |    D |    L |    L |\n",
      " |     U |    D |    D |    U |    U |    R |    D |    D |    R |    U |    U |    D |    L |    D |    R |    U |    U |    D |    L |    R |    L |    R |    U |    D |    L |    L |    R |    D |    R |    U |    U |    D |    R |    L |    D |    R |    D |    R |    R |    L |    R |    U |    L |    D |    R |    R |    L |    D |    L |    L |\n",
      " |     U |    U |    L |    R |    D |    D |    L |    L |    R |    D |    R |    U |    U |    D |    D |    L |    U |    L |    L |    R |    U |    L |    R |    R |    L |    L |    R |    U |    D |    U |    R |    D |    D |    R |    U |    D |    D |    U |    D |    L |    R |    U |    L |    U |    U |    D |    R |    D |    R |    D |\n",
      " |     U |    D |    U |    L |    L |    L |    L |    D |    U |    L |    D |    R |    L |    R |    U |    U |    L |    R |    R |    D |    R |    U |    D |    U |    D |    L |    D |    L |    U |    L |    D |    L |    R |    R |    U |    U |    R |    U |    L |    R |    D |    L |    D |    L |    L |    L |    D |    U |    D |    D |\n",
      " |     R |    R |    L |    U |    R |    U |    L |    U |    R |    R |    U |    R |    D |    D |    D |    U |    L |    U |    D |    R |    L |    D |    L |    U |    U |    D |    L |    D |    U |    R |    R |    D |    U |    L |    U |    U |    L |    L |    L |    R |    R |    U |    D |    U |    L |    L |    U |    L |    U |    D |\n",
      " |     R |    U |    U |    D |    D |    L |    D |    L |    R |    L |    D |    U |    D |    U |    L |    L |    D |    R |    R |    L |    L |    R |    U |    U |    L |    U |    U |    L |    R |    L |    D |    L |    U |    R |    L |    U |    L |    U |    U |    U |    L |    D |    D |    R |    R |    D |    D |    D |    U |    D |\n",
      " |     D |    U |    R |    L |    U |    R |    R |    R |    U |    R |    U |    R |    R |    R |    D |    R |    R |    R |    U |    R |    R |    U |    R |    U |    R |    U |    L |    U |    U |    R |    L |    L |    D |    L |    U |    U |    U |    D |    D |    L |    U |    L |    R |    U |    R |    R |    D |    U |    U |    D |\n",
      " |     R |    D |    D |    D |    R |    U |    U |    U |    L |    U |    R |    U |    D |    L |    R |    L |    U |    D |    R |    L |    U |    D |    L |    D |    L |    U |    U |    R |    D |    L |    D |    D |    L |    D |    R |    L |    U |    D |    U |    D |    L |    R |    L |    U |    D |    D |    R |    L |    L |    D |\n",
      " |     U |    D |    D |    L |    U |    U |    U |    D |    D |    L |    D |    L |    D |    R |    L |    R |    U |    U |    D |    R |    U |    D |    D |    U |    R |    L |    R |    L |    U |    R |    U |    D |    U |    L |    R |    D |    L |    D |    L |    L |    U |    U |    L |    U |    U |    L |    L |    U |    R |    D |\n",
      " |     U |    D |    L |    U |    L |    U |    U |    R |    L |    R |    R |    U |    U |    D |    U |    L |    U |    L |    R |    U |    D |    U |    R |    L |    L |    U |    D |    D |    D |    U |    R |    R |    L |    D |    L |    R |    D |    R |    L |    R |    D |    U |    L |    D |    U |    R |    U |    R |    U |    L |\n",
      " |     U |    U |    D |    U |    D |    D |    L |    U |    U |    R |    U |    U |    U |    L |    U |    U |    L |    R |    D |    U |    U |    L |    D |    L |    R |    D |    L |    R |    D |    U |    D |    R |    R |    D |    L |    L |    L |    U |    U |    D |    U |    U |    D |    R |    U |    U |    L |    L |    D |    D |\n",
      " |     R |    R |    R |    L |    L |    U |    D |    L |    D |    D |    R |    U |    L |    D |    U |    L |    R |    D |    U |    U |    U |    R |    R |    D |    D |    D |    L |    R |    R |    L |    R |    L |    U |    U |    R |    D |    D |    R |    L |    U |    L |    R |    U |    R |    D |    L |    R |    L |    D |    L |\n",
      " |     U |    R |    U |    D |    U |    L |    L |    D |    D |    D |    R |    R |    L |    R |    U |    R |    U |    U |    L |    L |    D |    R |    D |    L |    D |    D |    U |    R |    D |    R |    L |    D |    R |    R |    D |    U |    D |    U |    D |    U |    L |    U |    L |    D |    L |    U |    D |    D |    D |    L |\n",
      " |     R |    R |    R |    L |    R |    R |    L |    R |    L |    R |    R |    L |    L |    U |    R |    R |    D |    R |    R |    D |    L |    L |    L |    L |    U |    U |    D |    R |    L |    R |    D |    L |    R |    L |    D |    U |    D |    U |    D |    L |    L |    D |    D |    R |    R |    U |    R |    R |    D |    L |\n",
      " |     R |    D |    U |    L |    U |    D |    U |    R |    L |    R |    R |    L |    R |    R |    D |    L |    R |    R |    R |    L |    D |    R |    R |    L |    D |    D |    U |    R |    D |    L |    L |    R |    R |    R |    R |    R |    D |    L |    R |    R |    D |    D |    R |    L |    D |    D |    R |    L |    D |    U |\n",
      " |     D |    U |    R |    U |    L |    U |    L |    D |    D |    U |    R |    L |    D |    L |    U |    D |    L |    U |    R |    R |    R |    R |    U |    U |    L |    D |    D |    L |    D |    U |    D |    L |    R |    L |    L |    D |    U |    L |    R |    L |    L |    R |    U |    R |    L |    R |    D |    U |    R |    D |\n",
      " |     D |    R |    U |    D |    L |    L |    L |    D |    U |    D |    R |    U |    R |    D |    D |    R |    R |    R |    L |    R |    U |    U |    R |    D |    D |    U |    L |    D |    D |    L |    U |    U |    R |    R |    L |    U |    U |    R |    U |    L |    R |    L |    U |    R |    U |    L |    R |    U |    R |    D |\n",
      " |     D |    U |    L |    L |    L |    L |    R |    D |    U |    D |    U |    D |    U |    L |    U |    D |    D |    R |    R |    D |    U |    L |    R |    L |    U |    D |    L |    U |    R |    D |    L |    D |    R |    D |    L |    L |    U |    L |    L |    D |    U |    U |    R |    R |    L |    L |    L |    D |    D |    U |\n",
      " |     U |    L |    D |    U |    R |    R |    R |    L |    R |    R |    L |    R |    L |    D |    L |    D |    U |    D |    R |    U |    R |    U |    D |    R |    D |    D |    U |    R |    U |    R |    R |    L |    L |    D |    L |    R |    D |    R |    U |    U |    D |    L |    U |    D |    L |    R |    D |    R |    U |    L |\n",
      " |     R |    R |    D |    L |    D |    R |    D |    L |    D |    U |    L |    U |    L |    R |    U |    U |    U |    R |    L |    L |    L |    D |    L |    U |    U |    R |    L |    R |    D |    U |    R |    U |    D |    R |    D |    R |    L |    U |    U |    U |    R |    U |    U |    D |    U |    R |    R |    R |    L |    D |\n",
      " |     R |    L |    U |    D |    R |    R |    U |    R |    L |    R |    R |    L |    D |    D |    R |    R |    U |    R |    U |    U |    R |    R |    R |    L |    U |    L |    R |    D |    L |    R |    U |    D |    R |    U |    R |    L |    R |    L |    R |    D |    L |    R |    R |    U |    D |    D |    U |    R |    U |    U |\n",
      " |     U |    L |    D |    R |    D |    D |    R |    R |    U |    D |    D |    R |    D |    U |    L |    L |    L |    L |    U |    D |    R |    L |    L |    R |    U |    R |    D |    L |    U |    R |    D |    D |    L |    U |    R |    D |    U |    U |    R |    L |    D |    R |    R |    D |    R |    R |    R |    R |    L |    L |\n",
      " |     D |    R |    R |    U |    U |    D |    L |    D |    R |    R |    L |    D |    U |    L |    D |    U |    R |    L |    R |    R |    R |    D |    R |    D |    R |    U |    U |    R |    D |    L |    R |    U |    L |    U |    R |    D |    L |    R |    R |    L |    D |    L |    R |    L |    D |    U |    U |    U |    L |    U |\n",
      " |     U |    R |    U |    L |    R |    L |    L |    D |    U |    R |    R |    D |    R |    D |    D |    R |    R |    D |    L |    R |    D |    R |    L |    L |    D |    D |    U |    U |    R |    L |    R |    D |    D |    D |    R |    D |    U |    R |    R |    L |    L |    R |    R |    L |    U |    D |    L |    D |    U |    U |\n",
      " |     U |    U |    R |    D |    U |    R |    D |    U |    R |    U |    L |    R |    U |    L |    L |    R |    R |    D |    L |    D |    D |    U |    D |    L |    U |    U |    D |    L |    D |    U |    R |    R |    U |    L |    U |    D |    D |    D |    U |    D |    R |    R |    U |    D |    U |    D |    U |    U |    U |    D |\n",
      " |     U |    D |    U |    D |    U |    D |    D |    R |    D |    D |    D |    D |    D |    R |    L |    D |    R |    U |    L |    R |    L |    R |    U |    D |    R |    L |    R |    D |    D |    R |    L |    U |    R |    U |    R |    D |    R |    R |    R |    R |    R |    D |    D |    L |    R |    L |    R |    R |    L |    L |\n",
      " |     U |    R |    R |    U |    U |    U |    R |    U |    L |    U |    R |    U |    U |    D |    L |    D |    U |    L |    U |    R |    L |    R |    D |    D |    L |    D |    D |    R |    L |    L |    D |    R |    R |    R |    L |    L |    L |    D |    U |    D |    L |    L |    R |    R |    L |    U |    R |    R |    D |    L |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " |     D |    R |    L |    U |    D |    U |    L |    R |    L |    L |    D |    L |    D |    R |    R |    R |    L |    D |    U |    U |    U |    D |    U |    U |    D |    U |    U |    L |    L |    L |    L |    D |    U |    L |    D |    U |    R |    L |    L |    D |    L |    D |    R |    L |    U |    L |    U |    U |    D |    D |\n",
      " |     R |    U |    U |    D |    R |    D |    R |    R |    D |    U |    D |    D |    U |    D |    D |    R |    U |    U |    D |    R |    L |    L |    D |    R |    L |    D |    R |    L |    U |    D |    D |    U |    L |    D |    L |    U |    R |    R |    D |    U |    L |    U |    R |    R |    D |    D |    D |    D |    R |    D |\n",
      " |     U |    U |    R |    R |    D |    U |    R |    D |    L |    L |    U |    R |    L |    U |    R |    U |    U |    U |    U |    L |    R |    U |    U |    L |    L |    L |    D |    L |    L |    R |    R |    R |    U |    D |    L |    D |    U |    R |    D |    U |    D |    L |    D |    L |    U |    L |    U |    D |    R |    U |\n",
      " |     R |    U |    R |    R |    D |    D |    L |    L |    R |    R |    L |    L |    L |    R |    D |    L |    D |    L |    U |    R |    U |    R |    U |    U |    L |    U |    R |    U |    R |    D |    D |    R |    D |    R |    R |    D |    R |    L |    D |    D |    U |    R |    R |    D |    L |    U |    R |    R |    D |    D |\n",
      " |     U |    L |    L |    U |    U |    R |    U |    D |    R |    D |    D |    U |    D |    U |    L |    U |    R |    U |    U |    D |    U |    D |    R |    L |    L |    D |    U |    D |    R |    D |    R |    R |    R |    D |    L |    R |    L |    R |    L |    R |    D |    L |    U |    U |    U |    R |    R |    L |    R |    L |\n",
      " |     U |    R |    R |    U |    R |    U |    U |    U |    R |    U |    U |    R |    U |    R |    L |    R |    L |    R |    R |    L |    L |    U |    U |    U |    U |    U |    U |    U |    R |    U |    R |    L |    U |    U |    L |    R |    U |    U |    R |    U |    L |    L |    U |    L |    R |    L |    R |    R |    U |    U |\n",
      "Computing V_i through the value function of aforementioned greedy policy ... ... .\n"
     ]
    }
   ],
   "source": [
    "S_t, S_b = Determine_States(50)\n",
    "Rewards = Setting_Rewards()\n",
    "Actions = Setting_Actions()\n",
    "V = Inital_value_function(S_b,S_t)\n",
    "e = 0.7\n",
    "Discount = 0.9\n",
    "g_Policy = greedy_policy_of_current_policy(S_b, S_t, V)\n",
    "P = Possible_Action_for_states(S_b,S_t)  # this is created for specific question\n",
    "V_S_Left, V_S_Right, Number_of_update_policy = Policy_Iteration(S_b, S_t, Rewards, Actions, g_Policy, V, Discount, P, e)\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# x axis values \n",
    "x = [i for i in range(Number_of_update_policy)] \n",
    "# corresponding y axis values \n",
    "y_1 = V_S_Left \n",
    "# plotting the points  \n",
    "plt.plot(x, y_1, label = \"Botton left\")  \n",
    "\n",
    "y_2 = V_S_Right\n",
    "plt.plot(x, y_2, label = \"Botton Right\")\n",
    "# naming the x axis \n",
    "plt.xlabel('number of updates') \n",
    "# naming the y axis \n",
    "plt.ylabel('value function') \n",
    "\n",
    "# giving a title to my graph \n",
    "plt.title('Value function of two bottom states.') \n",
    "\n",
    "# function to show the plot \n",
    "plt.show() \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(Number_of_update_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "P_Dynamic = Possible_Action_for_states(S_b,S_t)\n",
    "q = Dynamic_p([0,4],10,[0,3],\"R\", P_Dynamic)\n",
    "print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = Inital_value_function(S_b,S_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Printing_Deterministic_Policy(gp,S_t, S_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_p = greedy_policy_of_current_policy(S_b, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Printing_ValueFunction(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Printing_Deterministic_Policy(g_p,S_t, S_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Possible_Action_for_states(S_b,S_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'V_S_Left' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-6f8e65928aa1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mV_S_Left\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'V_S_Left' is not defined"
     ]
    }
   ],
   "source": [
    "V_S_Left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(S_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "t = random.uniform(0,1)\n",
    "print(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate list (not \"int\") to list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-cb6bd9f75d09>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: can only concatenate list (not \"int\") to list"
     ]
    }
   ],
   "source": [
    "a = [1,2,3,4]\n",
    "b = a+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
