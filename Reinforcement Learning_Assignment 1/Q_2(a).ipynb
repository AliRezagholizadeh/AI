{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Setting_Rewards():\n",
    "    \n",
    "    Rewards = [0, 1, 10]\n",
    "    \n",
    "    return Rewards\n",
    "\n",
    "def Setting_Actions():\n",
    "    Actions = [\"U\", \"D\", \"L\", \"R\"]\n",
    "    \n",
    "    return Actions\n",
    "\n",
    "\n",
    "def Convert_position_list_to_str(s):\n",
    "    # s is [,] position.\n",
    "    s_str = '['\n",
    "    s_str += ''.join(str(x) for x in s)\n",
    "    s_str += ']'\n",
    "    \n",
    "    return s_str\n",
    "def Convert_position_str_to_list(s_str):\n",
    "    \n",
    "    r = int(s_str[1])\n",
    "    c = int(s_str[2])\n",
    "    \n",
    "    s= [r,c]\n",
    "    \n",
    "    return s\n",
    "\n",
    "def greedy_policy_of_current_policy(S_b, S_t, V):\n",
    "    import random\n",
    "    Greedy_Policy = {}\n",
    "    for s in S_b:\n",
    "        Action_set = Deterministic_Actions(s,V)\n",
    "        if(Action_set == \"Error\"):\n",
    "            print(\"Error occurred in obtaining greedy policy in state: \", s)\n",
    "        else:\n",
    "            s_str = Convert_position_list_to_str(s)\n",
    "            if(len(Action_set) > 0):\n",
    "                n_a = len(Action_set)\n",
    "                direction_n = int(random.uniform(0,n_a))\n",
    "                direction = Action_set[direction_n]\n",
    "                d = {s_str: direction}\n",
    "                Greedy_Policy.update(d)\n",
    "                \n",
    "    for s in S_t:\n",
    "        s_str = Convert_position_list_to_str(s)\n",
    "        d = {s_str: \"\"}\n",
    "        Greedy_Policy.update(d)\n",
    "                \n",
    "        \n",
    "    \n",
    "    # Greedy_Policy  is a dict  like: {\"[00]\":'U'}\n",
    "    return Greedy_Policy\n",
    "    \n",
    "\n",
    "def Determine_States(n):\n",
    "    # This is determined according the requirements of the ass1 _ q2(a) in Comp 767.\n",
    "    # the states are position like [[,][,]...]\n",
    "    S_t = [[0,0] , [0,n-1]]\n",
    "    # S_t is terminal state.\n",
    "    S_b = [[i,j] for i in range(n) for j in range(n)]\n",
    "    S_b.pop(n-1)\n",
    "    S_b.pop(0)\n",
    "    # S_b is in between _ not terminal state.\n",
    "    \n",
    "    return S_t, S_b\n",
    "    \n",
    "\n",
    "def Inital_value_function(S_b,S_t):\n",
    "    # S_t  is terminal states,  and 'S_b'  is between sates. Each is list of positions. [[,] [,] ..]\n",
    "    import math\n",
    "    n1 = len(S_b)\n",
    "    n2 = len(S_t)\n",
    "    \n",
    "    n = n1+n2\n",
    "    width = int(math.sqrt(n))\n",
    "    Value_Function = []\n",
    "    for i in range(width):\n",
    "        r = [0 for j in range(width)]\n",
    "        Value_Function.append(r)\n",
    "    \n",
    "    return Value_Function\n",
    "    \n",
    "\n",
    "def Deterministic_Actions(s,V):\n",
    "    # this is deterministic action selection according to policy Pai.\n",
    "    # So, it select an action according to value function of that policy.\n",
    "    # input: s is a position of one point, list of two numbers (position in gride- which is starting from 0)    -  \n",
    "    #        ... V is Value function of a policy (is matrix of gride size).\n",
    "    import numpy as np\n",
    "    r = s[0]\n",
    "    c = s[1]\n",
    "    \n",
    "    All_logical_neighbors = [[r-1, c], [r+1 , c], [r, c-1] , [r, c+1]]\n",
    "    end_of_grid = len(V)\n",
    "    Possible_neighbors = [z for z in All_logical_neighbors if(z[0]>-1 and z[0]< end_of_grid and z[1]>-1 and z[1]< end_of_grid)]\n",
    "    max_neighbor_value = -10\n",
    "    \n",
    "    for position in Possible_neighbors:\n",
    "        value = V[position[0]][position[1]]\n",
    "        if(max_neighbor_value < value):\n",
    "            max_neighbor_value = value\n",
    "    \n",
    "    max_neighbor_pos = []\n",
    "    for position in Possible_neighbors:\n",
    "        value = V[position[0]][position[1]]\n",
    "        if(max_neighbor_value == value):\n",
    "            max_neighbor_pos.append(position)\n",
    "    # all neighbors who have the max value would enter to 'max_neighbor_pos'.\n",
    "    \n",
    "    action_set = \"\"\n",
    "    s_array = np.array(s)\n",
    "    for max_Neigh in max_neighbor_pos:\n",
    "        max_neighbor_arr = np.array(max_Neigh)\n",
    "        move = max_neighbor_arr - s_array\n",
    "    \n",
    "        if(move[0] == -1):\n",
    "            action_set += \"U\"\n",
    "        elif(move[0] == 1):\n",
    "            action_set += \"D\"\n",
    "        else:\n",
    "            if(move[0] != 0):\n",
    "                print(\"Error in Choosing_deterministic_Action, 1.\", move[0])\n",
    "                action_set = \"Error\"\n",
    "\n",
    "\n",
    "        if(move[1] == -1):\n",
    "            action_set += \"L\"\n",
    "        elif(move[1] == 1):\n",
    "            action_set += \"R\"\n",
    "        else:\n",
    "            if(move[1] != 0):\n",
    "                print(\"Error in Choosing_deterministic_Action, 2.\" , move[1])\n",
    "                action_set = \"Error\"\n",
    "\n",
    "\n",
    "    \n",
    "    return action_set\n",
    "    \n",
    "\n",
    "def Printing_ValueFunction(matrix_list):\n",
    "    # 'matrix_list' is [[,,..][,,..]...]\n",
    "    r_len = len(matrix_list)\n",
    "    c_len = len(matrix_list[0])\n",
    "    print(\"Value Function:\", end ='\\n')\n",
    "    for i in range(r_len):\n",
    "        for j in range(c_len):\n",
    "            if(j==0):\n",
    "                print(end=' | ')\n",
    "                print(matrix_list[i][j], end=' | ')\n",
    "            else:\n",
    "                print(matrix_list[i][j], end=' | ')\n",
    "        print('')\n",
    "    return\n",
    "\n",
    "def Printing_Deterministic_Policy(g_p,S_t, S_b):\n",
    "    # g_p is dict of like {'[11]': \"UDLR\"}\n",
    "    import math\n",
    "    n1 = len(S_b)\n",
    "    n2 = len(S_t)\n",
    "    \n",
    "    n = n1+n2\n",
    "    width = int(math.sqrt(n))\n",
    "\n",
    "    Greedy_Policy_mat = []\n",
    "    for i in range(width):\n",
    "        r = ['' for j in range(width)]\n",
    "        Greedy_Policy_mat.append(r)  \n",
    "    \n",
    "    for s in S_b:\n",
    "        s_str = Convert_position_list_to_str(s)\n",
    "        Greedy_Policy_mat[s[0]][s[1]] = g_p[s_str]\n",
    "        \n",
    "    \n",
    "    for s in S_t:\n",
    "        #s_str = Convert_position_list_to_str(s)\n",
    "        Greedy_Policy_mat[s[0]][s[1]] = \"term\"\n",
    "     \n",
    "\n",
    "    # printing \n",
    "    matrix_list = Greedy_Policy_mat\n",
    "    r_len = len(matrix_list)\n",
    "    c_len = len(matrix_list[0])\n",
    "\n",
    "    #'value %3s - num of occurances = %d' % item\n",
    "    #'{0:10} ==> {1:10d}'.format(name, phone)\n",
    "    \n",
    "    \n",
    "    for i in range(r_len):\n",
    "        for j in range(c_len):\n",
    "            if(j==0):\n",
    "                print(end=' | ')\n",
    "                print(\"%5s |\"% matrix_list[i][j], end='')\n",
    "            else:\n",
    "                print(\"%5s |\"% matrix_list[i][j], end='')\n",
    "        print('')\n",
    "    \n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "#==============================================================================================\n",
    "# DP part:\n",
    "# \n",
    "\n",
    "\n",
    "def Policy_Iteration(S_b, S_t, Rewards, Actions, Policy, V, Discount, P_Dynamic, e):\n",
    "    import math\n",
    "    Greedy_Policy = Policy\n",
    "    # Greedy_Policy  is dict of like {'[12]':'U'}\n",
    "    Theta = 0.001\n",
    "    \n",
    "    S_p = S_b + S_t\n",
    "    n = len(S_p)\n",
    "    n = int(math.sqrt(n))\n",
    "    policy_stable = False\n",
    "    # S_p is s+\n",
    "    Number_of_update_policy = 1\n",
    "    V_S_Left = []\n",
    "    V_S_Right = []\n",
    "    while(policy_stable == False):\n",
    "        \n",
    "        print(\"===================================== update: \",Number_of_update_policy,\" ==================================\")\n",
    "        print(\"Greedy policy of current policy: \\n\")\n",
    "        Printing_Deterministic_Policy(Greedy_Policy,S_t, S_b)\n",
    "        \n",
    "        print(\"Computing V_i through the value function of aforementioned greedy policy ... ... .\")\n",
    "        Delta = 2\n",
    "        while(Theta < Delta):\n",
    "            Delta = 0\n",
    "            for s in S_b:\n",
    "                # s is [r,c] as the position of current state.\n",
    "                r_s = s[0]\n",
    "                c_s = s[1]\n",
    "                current_v = V[r_s][c_s]\n",
    "                s_str = Convert_position_list_to_str(s)\n",
    "                greedy_action = Greedy_Policy[s_str]\n",
    "                \n",
    "                Greedy_Policy_ = E_greedy_policy(Greedy_Policy, S_b, Actions, e)\n",
    "                actions, p_ = Greedy_Policy_[s_str]\n",
    "                if(greedy_action != \"\"):\n",
    "                    # when s is not terminal\n",
    "                    sum__ = 0\n",
    "                    for a_i in range(len(Actions)):\n",
    "                        pi = p_[a_i]\n",
    "                        ac = Actions[a_i]\n",
    "                        sum_ = 0\n",
    "                        for s_ in S_p:\n",
    "                            for r in Rewards:\n",
    "                                p_dy = Dynamic_p(s_,r,s,ac, P_Dynamic)\n",
    "                                if(p_dy != 0):\n",
    "                                    #print(\"s, greedy_action, S_, r, p\",s, greedy_action, s_, r , p_dy)\n",
    "                                    sum_ += p_dy*(r + Discount* V[s_[0]][s_[1]])\n",
    "                        sum__ += pi * sum_\n",
    "\n",
    "                    new_v = sum__\n",
    "                    V[r_s][c_s] = new_v\n",
    "                    diff = abs(current_v - new_v)\n",
    "                    #print(\"diff: \", diff, end=' - ')\n",
    "                    Delta = max(Delta , diff)\n",
    "            #print()\n",
    "            #print(\"Theta and Delta: \",Theta , Delta, end=' - ')\n",
    "        print(\"Value_function obtained from 'Iterative policy evaluation' is: \")\n",
    "        Printing_ValueFunction(V)\n",
    "        V_S_Left.append(V[n-1][0])\n",
    "        V_S_Right.append(V[n-1][n-1])\n",
    "        print(\"---------------------\")\n",
    "        print(\"Now updating greedy policy using this Value_function achieved ... ..\")\n",
    "        policy_stable = True\n",
    "        for s in S_b:\n",
    "            s_str = Convert_position_list_to_str(s)\n",
    "            current_action = Greedy_Policy[s_str]\n",
    "            \n",
    "            max_action_value = 0\n",
    "            max_q_action = \"\"\n",
    "            for a in Actions:\n",
    "                sum_ = 0\n",
    "                for s_ in S_p:\n",
    "                    for r in Rewards:\n",
    "                        p_dy = Dynamic_p(s_,r,s,a, P_Dynamic)\n",
    "                        if(p_dy != 0):\n",
    "                            sum_ += p_dy*(r + Discount* V[s_[0]][s_[1]])\n",
    "                \n",
    "                if(max_action_value <= sum_):\n",
    "                    max_action_value = sum_\n",
    "                    max_q_action = a\n",
    "                    \n",
    "            \n",
    "            Greedy_Policy[s_str] = max_q_action\n",
    "            \n",
    "            if(current_action != max_q_action):\n",
    "                policy_stable = False\n",
    "            \n",
    "        print(\"Greedy policy of updated policy: \\n\")\n",
    "        Printing_Deterministic_Policy(Greedy_Policy,S_t, S_b)\n",
    "        if(policy_stable == False):\n",
    "            Number_of_update_policy += 1\n",
    "            print(\"Upadted policy is not stable so we go through obtaining its Value_fuction, then updating g_policy again. \")\n",
    "            print(\"=========================================================================================\")\n",
    "            print(\"|||||||||||||||||||||||||||||||||||||||\")\n",
    "        else:\n",
    "            print(\"No update was needed. The last one is the optimal policy.\")\n",
    "\n",
    "\n",
    "    \n",
    "        \n",
    "    return V_S_Left, V_S_Right, Number_of_update_policy\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------\n",
    "def E_greedy_policy(Greedy_Policy, S_b, Actions, e):\n",
    "    Greedy_Policy_ = {}\n",
    "    for s in S_b:\n",
    "        s_str = Convert_position_list_to_str(s)\n",
    "        greedy_action = Greedy_Policy[s_str]\n",
    "        indx = 0\n",
    "        index = -1\n",
    "        for ac in Actions:\n",
    "            if(ac == greedy_action):\n",
    "                index = indx\n",
    "            indx += 1\n",
    "        p = [0 for i in range(len(Actions))]\n",
    "        p[index] = e\n",
    "        e_ = (1-e)/4\n",
    "        p_ = [i + e_ for i in p]\n",
    "        r = {s_str: [Actions, p_]}\n",
    "        Greedy_Policy_.update(r)\n",
    "    \n",
    "    return Greedy_Policy_\n",
    "def E_greedy(Greedy_Policy, S_b, Actions, e):\n",
    "    import random\n",
    "    \n",
    "    for s in S_b:\n",
    "        s_str = Convert_position_list_to_str(s)\n",
    "        greedy_action = Greedy_Policy[s_str]\n",
    "        \n",
    "        t = random.uniform(0,1)\n",
    "        if(t<e):\n",
    "            selection = greedy_action\n",
    "        else:\n",
    "            t_2 = random.uniform(0,1)\n",
    "            if(t_2 < 0.25):\n",
    "                selection = Actions[0]\n",
    "            elif(t_2 < 0.5):\n",
    "                selection = Actions[1]\n",
    "            elif(t_2 < 0.75):\n",
    "                selection = Actions[2]\n",
    "            else:\n",
    "                selection = Actions[3]\n",
    "        \n",
    "    return selection\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "# Regarding Dynamic:\n",
    "def Possible_Action_for_states(S_b,S_t):\n",
    "    import numpy as np\n",
    "    Possible_Actions_for_states = {}\n",
    "    terminal_1_str = Convert_position_list_to_str(S_t[0])\n",
    "    terminal_2_str = Convert_position_list_to_str(S_t[1])\n",
    "    for s in S_b:\n",
    "        r = s[0]\n",
    "        c = s[1]\n",
    "\n",
    "        All_logical_neighbors = [[r-1, c], [r+1 , c], [r, c-1] , [r, c+1]]\n",
    "        end_of_grid = len(V)\n",
    "        Possible_neighbors = [z for z in All_logical_neighbors if(z[0]>-1 and z[0]< end_of_grid and z[1]>-1 and z[1]< end_of_grid)]\n",
    "        max_neighbor_value = -10\n",
    "\n",
    "        s_array = np.array(s)\n",
    "        action_set = \"\"\n",
    "        Next_state_set =[]\n",
    "        Rewards = []\n",
    "        s_array = np.array(s)\n",
    "        for pos_N in Possible_neighbors:\n",
    "            pos_neighbor_arr = np.array(pos_N)\n",
    "            move = pos_neighbor_arr - s_array\n",
    "\n",
    "            if(move[0] == -1):\n",
    "                action_set += \"U\"\n",
    "                N_state_str = Convert_position_list_to_str(pos_N)\n",
    "                Next_state_set.append(N_state_str)\n",
    "                if(N_state_str == terminal_1_str):\n",
    "                    Rewards.append(1)\n",
    "                elif(N_state_str == terminal_2_str):\n",
    "                    Rewards.append(10)\n",
    "                else:\n",
    "                    Rewards.append(0)\n",
    "            elif(move[0] == 1):\n",
    "                action_set += \"D\"\n",
    "                N_state_str = Convert_position_list_to_str(pos_N)\n",
    "                Next_state_set.append(N_state_str)  \n",
    "                if(N_state_str == terminal_1_str):\n",
    "                    Rewards.append(1)\n",
    "                elif(N_state_str == terminal_2_str):\n",
    "                    Rewards.append(10)\n",
    "                else:\n",
    "                    Rewards.append(0)\n",
    "            else:\n",
    "                if(move[0] != 0):\n",
    "                    print(\"Error in Choosing_deterministic_Action, 1.\", move[0])\n",
    "                    action_set = \"Error\"\n",
    "\n",
    "\n",
    "            if(move[1] == -1):\n",
    "                action_set += \"L\"\n",
    "                N_state_str = Convert_position_list_to_str(pos_N)\n",
    "                Next_state_set.append(N_state_str) \n",
    "                if(N_state_str == terminal_1_str):\n",
    "                    Rewards.append(1)\n",
    "                elif(N_state_str == terminal_2_str):\n",
    "                    Rewards.append(10)\n",
    "                else:\n",
    "                    Rewards.append(0)                \n",
    "            elif(move[1] == 1):\n",
    "                action_set += \"R\"                \n",
    "                N_state_str = Convert_position_list_to_str(pos_N)\n",
    "                Next_state_set.append(N_state_str)     \n",
    "                if(N_state_str == terminal_1_str):\n",
    "                    Rewards.append(1)\n",
    "                elif(N_state_str == terminal_2_str):\n",
    "                    Rewards.append(10)\n",
    "                else:\n",
    "                    Rewards.append(0)\n",
    "            else:\n",
    "                if(move[1] != 0):\n",
    "                    print(\"Error in Choosing_deterministic_Action, 2.\" , move[1])\n",
    "                    action_set = \"Error\"\n",
    "        \n",
    "        s_str = Convert_position_list_to_str(s)\n",
    "        state_action = {s_str:[action_set, Next_state_set, Rewards]}\n",
    "        Possible_Actions_for_states.update(state_action)\n",
    "    for s in S_t: \n",
    "        action_set = \"\"\n",
    "        Next_state_set = []\n",
    "        Rewards = []\n",
    "        s_str = Convert_position_list_to_str(s)\n",
    "        state_action = {s_str:[action_set, Next_state_set, Rewards]}\n",
    "        Possible_Actions_for_states.update(state_action)\n",
    "    return Possible_Actions_for_states\n",
    "\n",
    "def Dynamic_p(s_,r,s,a , P):\n",
    "    # s is like [1,1] ; a is like \"U\" ; r is like 0 ; s_ is like [3,3]\n",
    "    # P is like: {'[01]':[Actions, Next_state, Rewards]}  \n",
    "    # S_b\n",
    "    s_str = Convert_position_list_to_str(s)\n",
    "    Actions, Next_state, Rewards = P[s_str]\n",
    "    #print(Actions)\n",
    "    indx = 0\n",
    "    index = -1\n",
    "    for ac in Actions:\n",
    "        if(ac == a):\n",
    "            index = indx\n",
    "        indx += 1\n",
    "    \n",
    "    # now index is corresponding action to take, next state, next reward considering this action.\n",
    "    if(index == -1):\n",
    "        return 0\n",
    "    s__str = Convert_position_list_to_str(s_)\n",
    "    if(Next_state[index] != s__str):\n",
    "        return 0\n",
    "    if(Rewards[index] != r):\n",
    "        return 0\n",
    "    \n",
    "    return 1\n",
    "    \n",
    "    # for each 'current state', 'action' and 'reward' , 'new state'  it should return the dynamic  of \n",
    "# -------------------------------------------------------------------------------------------------\n",
    "\n",
    "def Value_Iteration():\n",
    "    \n",
    "    return\n",
    "\n",
    "def Modified_Policy_Iteration():\n",
    "    \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_t, S_b = Determine_States(5)\n",
    "V = Inital_value_function(S_b,S_t)\n",
    "g_Policy = greedy_policy_of_current_policy(S_b, S_t, V)\n",
    "Printing_Deterministic_Policy(g_Policy,S_t, S_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_for_run(g_Policy):\n",
    "    S_t, S_b = Determine_States(5)\n",
    "    Rewards = Setting_Rewards()\n",
    "    Actions = Setting_Actions()\n",
    "    V = Inital_value_function(S_b,S_t)\n",
    "    e = 0.7\n",
    "    Discount = 0.9\n",
    "    P = Possible_Action_for_states(S_b,S_t)  # this is created for specific question\n",
    "    V_S_Left, V_S_Right, Number_of_update_policy = Policy_Iteration(S_b, S_t, Rewards, Actions, g_Policy, V, Discount, P, e)\n",
    "    \n",
    "    import matplotlib.pyplot as plt \n",
    "  \n",
    "    # x axis values \n",
    "    x = [i-1 for i in range(Number_of_update_policy)] \n",
    "    # corresponding y axis values \n",
    "    y_1 = V_S_Left \n",
    "    # plotting the points  \n",
    "    plt.plot(x, y_1, label = \"Botton left\")  \n",
    "\n",
    "    y_2 = V_S_Right\n",
    "    plt.plot(x, y_2, label = \"Botton Right\")\n",
    "    # naming the x axis \n",
    "    plt.xlabel('number of updates') \n",
    "    # naming the y axis \n",
    "    plt.ylabel('value function') \n",
    "\n",
    "    # giving a title to my graph \n",
    "    plt.title('Value function of two bottom states.') \n",
    "\n",
    "    # function to show the plot \n",
    "    plt.show() \n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_Policy_ = g_Policy\n",
    "main_for_run(g_Policy_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Main():\n",
    "    S_t, S_b = Determine_States(50)\n",
    "    Rewards = Setting_Rewards()\n",
    "    Actions = Setting_Actions()\n",
    "    V = Inital_value_function(S_b,S_t)\n",
    "    e = 0.7\n",
    "    Discount = 0.9\n",
    "    g_Policy = greedy_policy_of_current_policy(S_b, S_t, V)\n",
    "    P = Possible_Action_for_states(S_b,S_t)  # this is created for specific question\n",
    "    V_S_Left, V_S_Right, Number_of_update_policy = Policy_Iteration(S_b, S_t, Rewards, Actions, g_Policy, V, Discount, P, e)\n",
    "\n",
    "    import matplotlib.pyplot as plt \n",
    "\n",
    "    # x axis values \n",
    "    x = [i for i in range(Number_of_update_policy)] \n",
    "    # corresponding y axis values \n",
    "    y_1 = V_S_Left \n",
    "    # plotting the points  \n",
    "    plt.plot(x, y_1, label = \"Botton left\")  \n",
    "\n",
    "    y_2 = V_S_Right\n",
    "    plt.plot(x, y_2, label = \"Botton Right\")\n",
    "    # naming the x axis \n",
    "    plt.xlabel('number of updates') \n",
    "    # naming the y axis \n",
    "    plt.ylabel('value function') \n",
    "\n",
    "    # giving a title to my graph \n",
    "    plt.title('Value function of two bottom states.') \n",
    "\n",
    "    # function to show the plot \n",
    "    plt.show() \n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Number_of_update_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_Dynamic = Possible_Action_for_states(S_b,S_t)\n",
    "q = Dynamic_p([0,4],10,[0,3],\"R\", P_Dynamic)\n",
    "print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = Inital_value_function(S_b,S_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Printing_Deterministic_Policy(gp,S_t, S_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_p = greedy_policy_of_current_policy(S_b, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Printing_ValueFunction(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Printing_Deterministic_Policy(g_p,S_t, S_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Possible_Action_for_states(S_b,S_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_S_Left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(S_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "t = random.uniform(0,1)\n",
    "print(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3,4]\n",
    "b = a+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
